---
title       : Training options
subtitle    : 
author      : Jeffrey Leek
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../librariesNew
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
knitr::opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knitr::knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knitr::knit_hooks$set(plot = knitr:::hook_plot_html)
```


## SPAM Example

```{r loadPackage,cache=TRUE}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
modelFit <- train(type ~.,data=training, method="glm")
```

---

## Train options

The options you can look at ?train.default. Possibilities:

+ preProcess - singular value decomposition for instance, or normalization 
+ weights - use when you have unbalanced training sets, where you have a lot more observations of one type than another
+ metric - defaults are Accuracy, i.e. fraction that is correct, for factor variables (Kappa is an alternative) and RMSE for continous variables ($R^2$ alternative)
+ trControl


---

## Metric options

__Continous outcomes__:

 * _RMSE_ = Root mean squared error
 * _RSquared_ = $R^2$ from regression models

__Categorical outcomes__:

 * _Accuracy_ = Fraction correct
 * _Kappa_ = A measure of [concordance](http://en.wikipedia.org/wiki/Cohen%27s_kappa)
  
 
--- 

## trainControl

```{r , dependson="loadPackage",cache=TRUE}
args(trainControl)
```

--- 

## trainControl resampling

* method
    + _boot_ = bootstrapping
    + _boot632_ = bootstrapping with adjustment - will reduce some of the bias due to bootsrtraping
    + _cv_ = cross validation
    + _repeatedcv_ = repeated cross validation
    + _LOOCV_ = leave one out cross validation
* number
    + For boot/cross validation
    + Number of subsamples to take
* repeats
    + Number of times to repeate subsampling - for repeated cross validation
    + If big this can _slow things down_
* p
    + size of the training set
* initial Window
    + used for time course data, tells you the size of the training data set
* horizon
    + the number of time points you will be predicting
* savePredictions
    + return actual predictions from each itteration
* summaryFunction
    + different types of summary outputs
* seeds
    + set seeds for all the different resampling layers
    
    
    
    


---

## Setting the seed

* It is often useful to set an overall seed
* You can also set a seed for each resample
* Seeding each resample is useful for parallel fits



--- 


## seed example

```{r , dependson="seedExample",cache=TRUE}
set.seed(1235)
modelFit2 <- train(type ~.,data=training, method="glm")
modelFit2
```


--- 

## seed example

```{r , dependson="seedExample",cache=TRUE}
set.seed(1235)
modelFit3 <- train(type ~.,data=training, method="glm")
modelFit3
```


--- 

## Further resources

* [Caret tutorial](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
* [Model training and tuning](http://caret.r-forge.r-project.org/training.html)


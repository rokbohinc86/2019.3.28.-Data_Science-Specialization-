---
title: "Quiz 3"
author: "Rok Bohinc"
date: "July 11, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

```{r message=FALSE, warning=FALSE}
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
```

1. Subset the data to a training set and testing set based on the Case variable in the data set.

```{r}
#inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
library(dplyr)
training <- segmentationOriginal[segmentationOriginal$Case=="Train",] %>% select(-c(Case))
testing <- segmentationOriginal[segmentationOriginal$Case=="Test",] %>% select(-c(Case))
```

2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings.

```{r}
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
```


3. In the final model what would be the final model prediction for cases with the following variable values:


a. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2 -> PS with 93%

b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100 -> WS

c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100 -> PS

d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2 -> Not possible

```{r message=FALSE, warning=FALSE}
library(rattle)
fancyRpartPlot(modFit$finalModel)
print(modFit$finalModel)
```

## Question 2

If K is small in a K-fold cross validation is the bias in the estimate of out-of-sample (test set) accuracy smaller or bigger? If K is small is the variance in the estimate of out-of-sample (test set) accuracy smaller or bigger. Is K large or small in leave one out cross validation?

The bias is larger (because you did a small amount of samples) and the variance is smaller (because you did a small amount of samples). 
Under leave one out cross validation K is equal to the sample size.

## Question 3

```{r}
library(pgmm)
data(olive)
olive = olive[,-1]
```

These data contain information on 572 different Italian olive oils from multiple regions in Italy. Fit a classification tree where Area is the outcome variable. Then predict the value of area for the following data frame using the tree command with all defaults

```{r}

modFit <- train(Area ~ .,method="rpart",data=olive)
fancyRpartPlot(modFit$finalModel)

newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata=newdata)
```

## Question 4

```{r}
library(ElemStatLearn)
data(SAheart)
#SAheart$chd <- as.factor(SAheart$chd)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
```

Then set the seed to 13234 and fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors. Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:

What is the misclassification rate on the training set? What is the misclassification rate on the test set?

```{r}
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
modFitlm <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, family="binomial")


missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(modFit, trainSA))
missClass(testSA$chd, predict(modFit, testSA))

missClass(trainSA$chd, predict(modFitlm, trainSA))
missClass(testSA$chd, predict(modFitlm, testSA))
```

Interestingly non of the following answers are correct:

* Test Set Misclassification: 0.27; Training Set: 0.31
* Test Set Misclassification: 0.35; Training Set: 0.31
* Test Set Misclassification: 0.43; Training Set: 0.31

## Question 5

```{r}
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833.)
```

Set the variable y to be a factor variable in both the training and test set. Then set the seed to 33833. Fit a random forest predictor relating the factor variable y to the remaining variables. Read about variable importance in random forests here: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr The caret package uses by default the Gini importance.

```{r, cache=TRUE}
modFit <- train(y~ .,data=vowel.train,method="rf",prox=TRUE)
```


Calculate the variable importance using the varImp function in the caret package. What is the order of variable importance?

```{r}
varImp(modFit)
```


[NOTE: Use randomForest() specifically, not caret, as there's been some issues reported with that approach. 11/6/2016]